---
title: AI Digital Twin
description: An intelligent RAG-powered agent that acts as a digital extension of myself, answering questions about my work, skills, and background.
imagePath: "/src/images/cover/ai-agent-cover.png"
metaPath: "/images/covers/ai-agent-cover.png"
tags: ["Astro", "React", "RAG", "Vercel AI SDK", "Gemini"]
date: January 10, 2026
---

import Mermaid from '../../components/Mermaid';

<video src="/videos/ai-agent-demo.mp4" className="w-full rounded-lg shadow-lg mb-8" controls playsInline autoPlay muted loop />

## The Vision: A Digital Extension

Portfolios are static. They wait for you to read them. I wanted a portfolio that *talks back*. 
This project isn't just a chatbot; it's an **Agentic Digital Twin** designed to understand my entire body of work and answer questions as if I were sitting right there.

## Architecture

The system is built on a **Hybrid RAG (Retrieval-Augmented Generation)** architecture, leveraging the speed of static sites with the power of serverless AI.

### The Stack
- **Framework**: Astro (Hybrid rendering)
- **UI**: React 19 + Tailwind CSS
- **AI Engine**: Vercel AI SDK + Google Gemini 2.5 Flash
- **Vector Database**: Local JSON Vector Store (Optimized for edge)
- **Embeddings**: 'text-embedding-004'

<Mermaid client:idle chart={`flowchart TD
    subgraph Client ["Client (Browser)"]
        UI[Chat Interface]
        User[User]
        User -->|Asks Question| UI
    end

    subgraph API ["Vercel Serverless Function"]
        Endpoint["/api/chat"]
        
        subgraph Security ["Security Layer"]
            RateLimit["Rate Limiter (IP-based)"]
            Validator["Input Validator & Sanitizer"]
        end
        
        subgraph Processing ["Core Processing"]
            Embedder["Embedding Model\n(text-embedding-004)"]
            Search["Vector Search\n(Cosine Similarity)"]
            Router["Mention Router\n(@work, @blog detectors)"]
        end
        
        subgraph LLM ["Generation Layer"]
            Gemini["Google Gemini 2.5 Flash"]
            SysPrompt["System Prompt\n(Context + Rules + Profile)"]
        end
    end

    subgraph Data ["Knowledge Base"]
        Vectors[("Vector Store JSON")]
        Content["MDX Content\n(Blogs & Work)"]
    end

    %% Flow Connections
    UI -->|POST Request| Endpoint
    Endpoint --> RateLimit
    RateLimit -->|Allowed| Validator
    Validator -->|Valid| Embedder
    
    Embedder -->|Query Vector| Router
    Router -->|Filtered Scope| Search
    
    Data -->|Loading Context| Search
    Search -->|Top 5 Chunks| SysPrompt
    
    SysPrompt -->|Augmented Prompt| Gemini
    Gemini -->|Streaming Response| UI

    %% Styling
    style Gemini fill:#E8F0FE,stroke:#1a73e8,stroke-width:2px
    style Vectors fill:#FFF8E1,stroke:#FBC02D,stroke-width:2px
    style UI fill:#F3E5F5,stroke:#8E24AA,stroke-width:2px`} />

### Key Components

1.  **Vector Store**: Instead of a heavy external DB like Pinecone, I built a custom, lightweight JSON vector store that compiles at build time. This ensures 0ms network latency for retrievals.
2.  **Streaming API**: The `api/chat.ts` endpoint uses Vercel's AI SDK to stream responses token-by-token, giving it a "real-time" feel.
3.  **Smart Context**: The agent doesn't just keyword match. It uses cosine similarity combined with metadata filtering (e.g., detecting if you are asking about a 'blog' or 'work' item) to pull the perfect context.

### Technical Deep Dive

1. **Zero-Latency Retrieval**

   Most RAG apps hit a database like Pinecone or Qdrant for every query, adding 200-500ms of latency. I moved this **"Left" to build time**.
   - **Build Time**: A custom script scans all `.mdx` content, generates embeddings via `text-embedding-004`, and compiles them into a optimized `vector-store.json`.
   - **Runtime**: The API simply reads this local JSON file. No external database calls. No cold starts.

2. **Deterministic Intent Routing**

   LLMs are probabilistic, but sometimes you need deterministic behavior. I implemented a router that intercepts `@mentions` (like `@blog` or `@work`) before the semantic search.
   - If you ask "show me your @work", the system bypasses semantic search for that category and forces a "Collection Summary" into the context window, guaranteeing accurate retrieval.

3. **The "Anti-Robot" System Prompt**

   To prevent the agent from sounding like a generic support bot, the system prompt strictly forbids "meta-commentary".
   - **Banned**: "Based on the context...", "According to your portfolio..."
   - **Enforced**: Direct answers. "I built this using..." instead of "Anirban built this using..." when acting as the persona.

## Design Decisions & "Real Experience Score"

Speed is a feature. A chatbot that slows down the site is a failure.

### 1. Lazy Hydration Strategy
I implemented a `client:idle` hydration strategy for the chat widget. The main content loads instantly, and the heavy React chat code only hydrates once the main thread is free. providing a **100/100 Real Experience Score**.

### 2. Optimistic UI & Local State
The chat feels instant because handles all state locally and updates the UI optimistically before the server even responds.

### 3. Safety & Control
- **Rate Limiting**: Custom token-bucket rate limiting based on IP.
- **Input Sanitization**: rigorous regex patterns to prevent prompt injection.
- **Cost Controls**: Token usage tracking and limits.

## The Future

This is just V1. The next phase involves giving the agent **Tools**â€”the ability to navigate the site for you, switch themes, or even send me an email directly.
